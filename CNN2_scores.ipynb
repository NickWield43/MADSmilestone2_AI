{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NickWield43/MADSmilestone2_AI/blob/main/CNN2_scores.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa6L-qwlfHPf"
      },
      "source": [
        "# Supervised Learning: Clock Drawing Image Classification with Convolutional Neural Networks\n",
        "### Stacey Beck and Ian Byrne\n",
        "\n",
        "- Split data into sets of Training (x = image arrays ; y = labels), Test (~10% image arrays), and Validation (~10% of the Training).\n",
        "- Build CNN using Pytorch for Training and Test:\n",
        "  - Specify CUDA\n",
        "  - 2D convolution, Normalization (for faster training), Non-linear Activation Function (ex. RELU), Max Pooling (downsampling to reduce learned parameters).\n",
        "  - Define Layers\n",
        "  - Build Forward and backward pass\n",
        "  - Define optimizer (due to many - deep - nodes) ex) ADAM\n",
        "  - Calculate Loss (BCE)\n",
        "  - Calculate Accuracy, Precision, Recall (Confusion Matrix)\n",
        "  - Plot ROC and print Confusion Matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yd2RExPwkkd"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NickWield43/MADSmilestone2_AI/blob/main/CNN2_scores.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MKcEhDBuf5ow",
        "outputId": "c86d6c3a-b183-42b7-bdd5-687909e4ae79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/NickWield43/MADSmilestone2_AI.git\n",
        "\n",
        "# Change directory into cloned repo\n",
        "%cd MADSmilestone2_AI\n",
        "# List repo contents\n",
        "!ls"
      ],
      "metadata": {
        "id": "J2cFOxSfyITa",
        "outputId": "c6c3a4f3-e5ad-4cc4-e4eb-b073f23a2d57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MADSmilestone2_AI'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 117, done.\u001b[K\n",
            "remote: Counting objects: 100% (117/117), done.\u001b[K\n",
            "remote: Compressing objects: 100% (84/84), done.\u001b[K\n",
            "remote: Total 117 (delta 46), reused 99 (delta 30), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (117/117), 5.46 MiB | 10.10 MiB/s, done.\n",
            "Resolving deltas: 100% (46/46), done.\n",
            "/content/MADSmilestone2_AI\n",
            "Analysis.ipynb\t\t\tmultimodel1_updated.ipynb\n",
            "BECK_S_BYRNE_I_Final_695.pdf\tmultimodel2.ipynb\n",
            "CNN2_scores.ipynb\t\tpresentation_CNN2_scores.ipynb\n",
            "Data\t\t\t\tpresentation_Supervised_CNN.ipynb\n",
            "dataloader_tests.ipynb\t\tPresentation_Transfer_Learning_Kmeans.ipynb\n",
            "dataloader_tests_updated.ipynb\tproj_models.py\n",
            "ImagePlayground\t\t\tREADME.md\n",
            "Labeling\t\t\tscores_cnn_resnet.ipynb\n",
            "load_data.py\t\t\tscores_cnn_resnet_updated.ipynb\n",
            "Loading\t\t\t\tSupervised_CNN.ipynb\n",
            "Model_Datasets.ipynb\t\tTransfer_Learning_Kmeans.ipynb\n",
            "multimodel1.ipynb\t\tutils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Rounds data\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "AIdD-R23yasb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "XtH9tfFdQepN"
      },
      "outputs": [],
      "source": [
        "# General Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import logging\n",
        "import os\n",
        "import tempfile\n",
        "import glob\n",
        "\n",
        "# Pytroch Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "import torchmetrics\n",
        "\n",
        "# To Evaluate model\n",
        "from tqdm import tqdm\n",
        "import torchmetrics\n",
        "from torchmetrics import ConfusionMatrix\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# To visualize model\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from skimage.io import imread\n",
        "\n",
        "# To split the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n"
      ],
      "metadata": {
        "id": "vfHWp-Mb0RQV",
        "outputId": "de1f4f55-498f-4854-d678-338c92451e5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.1-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.8.0+cu126)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.19.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.8.1-py3-none-any.whl (982 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.0/983.0 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5jwtXk7qv9-"
      },
      "source": [
        "# Build CNN Model using Pytorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nKizV2h4lMH"
      },
      "source": [
        "### Building and Training\n",
        "Architecture choices influenced from:\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2018/12/guide-convolutional-neural-network-cnn/\n",
        "\n",
        "https://medium.datadriveninvestor.com/five-powerful-cnn-architectures-b939c9ddd57b\n",
        "\n",
        "https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7OOYf4s0smrq"
      },
      "outputs": [],
      "source": [
        "epochs = 2\n",
        "train_batch_size = 8\n",
        "val_batch = 4\n",
        "test_batch = 1\n",
        "learning_rate = 1e-3\n",
        "kernel_size = 3\n",
        "stride = 1\n",
        "padding = 1 #2*floor(3/2)\n",
        "weight_decay = 1e-5\n",
        "# Define file extension to use for new data saves\n",
        "extension_ = \"res_netfp\"\n",
        "# Normalize data if rgb and set rgb_val to True to convert\n",
        "normalize_ = 'True'\n",
        "# Define which round to get data from\n",
        "rnd = 7\n",
        "# Define model extensions for naming file (which model do we want to train on)\n",
        "model_ext = \"3\"\n",
        "# m = 'First model'\n",
        "# m = 'pre-trained'\n",
        "#m = 'pre-trained-res'\n",
        "m = 'resnet'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ypf_eiS8Eghk"
      },
      "outputs": [],
      "source": [
        "\n",
        "def open_dict(path):\n",
        "    \"\"\"Takes in a path and safely opens a dictionary file.\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"File not found: {path}\")\n",
        "\n",
        "    with open(path, \"r\") as cust_file:\n",
        "        contents = cust_file.read()\n",
        "        dictionary = ast.literal_eval(contents)\n",
        "    return dictionary\n",
        "\n",
        "# Use raw strings or forward slashes for paths\n",
        "dictionarytr = open_dict(\"Data/Dictionaries/score_dicts/tr_scor_dict_bal.txt\")\n",
        "dictionaryv = open_dict(\"Data/Dictionaries/score_dicts/val_scor_dict_nobal.txt\")\n",
        "dictionaryts = open_dict(\"Data/Dictionaries/score_dicts/tst_scor_dict_nobal.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "54Zv5Nsk-R-U",
        "outputId": "e7517e21-60ab-4c61-ae05-7a2b2f05e2ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round 9 length is 1936\n",
            "Round 7 length is 2413\n",
            "Round 8 length is 2211\n",
            "Round 4 length is 1740\n",
            "Round 2 length is 2617\n",
            "Round 10 length is 1464\n",
            "Round 5 length is 3367\n",
            "Round 6 length is 2740\n",
            "Round 3 length is 1976\n",
            "Round 1 length is 3536\n"
          ]
        }
      ],
      "source": [
        "for id, val in dictionarytr.items():\n",
        "  print(\"Round {} length is {}\".format(str(id), str(len(val))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EUo2KZ_UYhYE"
      },
      "outputs": [],
      "source": [
        "class ResizedClocks:\n",
        "    # Resized clock drawing dataset\n",
        "\n",
        "    def __init__(self, round, round_labels, rgb=None, transform=None):\n",
        "\n",
        "        # Args:\n",
        "        # round (int): Round to grab images from.\n",
        "        # values (list of tuples): Corresponding values for the round.\n",
        "\n",
        "        self.round = round\n",
        "        self.vals = round_labels\n",
        "        self.base_path = '/content/gdrive/MyDrive/Nhats Dataset/NHATS_R11_ClockDrawings_V2'\n",
        "        self.transform = transform\n",
        "        self.rgb = rgb\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vals)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        spid = self.vals[idx][0]\n",
        "        label = torch.tensor(int(self.vals[idx][1]))\n",
        "\n",
        "        # Try different file extensions\n",
        "        possible_files = [\n",
        "            f\"{self.base_path}/{spid}.tif\",\n",
        "            f\"{self.base_path}/{spid}.jpg\",\n",
        "            f\"{self.base_path}/{spid}.png\",\n",
        "            f\"{self.base_path}/{spid}.jpeg\"\n",
        "        ]\n",
        "\n",
        "        image_path = None\n",
        "        for file_path in possible_files:\n",
        "            if os.path.exists(file_path):\n",
        "                image_path = file_path\n",
        "                break\n",
        "\n",
        "        if image_path is None:\n",
        "            return None  # File not found\n",
        "\n",
        "        try:\n",
        "            im = Image.open(image_path)\n",
        "\n",
        "            if self.rgb:\n",
        "                gray = im.convert('RGB')\n",
        "            else:\n",
        "                gray = im.convert('1')\n",
        "\n",
        "            resized = gray.resize((160, 207))\n",
        "            im_arr = np.float32(np.array(resized))\n",
        "\n",
        "            if self.transform:\n",
        "                im_arr = self.transform(im_arr)\n",
        "\n",
        "            return im_arr, label\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {image_path}: {e}\")\n",
        "            return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Cco7mnKoYwEX"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "  \"\"\"From pytorch - way to bypass corrupt or non-existent data\"\"\"\n",
        "  batch = list(filter(lambda x: x is not None, batch))\n",
        "  return torch.utils.data.dataloader.default_collate(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Gn8N9D4YZJcR"
      },
      "outputs": [],
      "source": [
        "# initialize transformation: data to tensor and normalize\n",
        "# Could probably resize using torch.transforms\n",
        "if normalize_ == \"True\":\n",
        "    processes = transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        ]\n",
        "    )\n",
        "    rgb_val = \"True\"\n",
        "else:\n",
        "    processes = transforms.ToTensor()\n",
        "    rgb_val = None\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Bz4e3izRZMJx"
      },
      "outputs": [],
      "source": [
        "train_set = ResizedClocks(rnd, dictionarytr[rnd], transform = processes, rgb = rgb_val)\n",
        "val_set = ResizedClocks(rnd, dictionaryv[rnd], transform = processes, rgb = rgb_val)\n",
        "test_set = ResizedClocks(rnd, dictionaryts[rnd], transform = processes, rgb = rgb_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FPc4Tg8yZPiE"
      },
      "outputs": [],
      "source": [
        "# Define Dataloaders for the network\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size = train_batch_size, shuffle = True, num_workers = 6, collate_fn=collate_fn)\n",
        "validate_loader = torch.utils.data.DataLoader(val_set, batch_size = val_batch, shuffle = True, num_workers = 6, collate_fn=collate_fn) #64, 8,1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kne45tSsQvFQ"
      },
      "outputs": [],
      "source": [
        "# For round 10, there are some corrupt data that when batched at size 1 is not taken\n",
        "# care of by the collate function, but Nonechucks library skips the missing data and\n",
        "# moves on, replacing that missing data index with the next piece of data\n",
        "# could probably just use this in place of collate for all the loading\n",
        "if rnd == 10:\n",
        "  !pip install nonechucks\n",
        "  import nonechucks as nc\n",
        "  test_set_safe = nc.SafeDataset(test_set)\n",
        "  test_loader = torch.utils.data.DataLoader(test_set_safe, batch_size = test_batch, shuffle = False)\n",
        "else:\n",
        "  test_loader = torch.utils.data.DataLoader(test_set, batch_size = test_batch, shuffle = False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KAovKkRXFU4j"
      },
      "outputs": [],
      "source": [
        "def set_model(m, model_ext, device):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"First Model training on GPU\")\n",
        "\n",
        "        if m == \"First model\":\n",
        "            # Create model object\n",
        "            model = ConvNet()\n",
        "            model = model.to(device)  # (float).cuda()\n",
        "\n",
        "        elif m == \"pre-trained\":\n",
        "            mPATH = \"/content/gdrive/MyDrive/Colab Notebooks/Models/cnn_512_662.model{}\".format(\n",
        "                model_ext\n",
        "            )\n",
        "            model = ConvNet()\n",
        "            model.load_state_dict(torch.load(mPATH))\n",
        "            model.to(device)\n",
        "            print(\"New Model{} training on GPU\".format(model_ext))\n",
        "\n",
        "        elif m == \"resnet\":\n",
        "            model = models.resnet50(pretrained=True)\n",
        "            num_ftrs = model.fc.in_features\n",
        "            model.fc = nn.Linear(num_ftrs, 6)\n",
        "            model = model.to(device)\n",
        "            print(\"RESNET Model training on GPU\")\n",
        "\n",
        "        elif m == \"pre-trained-res\":\n",
        "            mPATH = \"/content/gdrive/MyDrive/Colab Notebooks/Models/cnn_512_662.model{}\".format(\n",
        "                model_ext\n",
        "            )\n",
        "            model = models.resnet50()\n",
        "            num_ftrs = model.fc.in_features\n",
        "            model.fc = nn.Linear(num_ftrs, 6)\n",
        "            model.load_state_dict(torch.load(mPATH))\n",
        "            model.to(device)\n",
        "            print(\"New Model{} training on GPU\".format(model_ext))\n",
        "\n",
        "    else:\n",
        "        print(\"CUDA is not available. Turn on GPU\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XiY5mGPbZxDV"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_pred, y_test):\n",
        "  # Calculating model accuracy at each epoch\n",
        "  y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
        "  _, y_pred_prob = torch.max(y_pred_softmax, dim = 1)\n",
        "  correct_pred = (y_pred_prob == y_test).float()\n",
        "  acc = correct_pred.sum() / len(correct_pred)\n",
        "  acc = torch.round(acc * 100)\n",
        "\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WrUu9AXiZ0SL"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        # without considering batch size: Input shape : (None,368, 284, 1) , parameters: (3*3*1*16+16) = 160\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=1,\n",
        "            out_channels=16,  # one input channel gray scale, 16 filters out\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "        )  # Out:(None,386, 284, 16). ### TRY kernel 7x7 padding 3\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1\n",
        "        )  # params: (3*3*16*32+32) = 4640\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)  # Out: (None, 184, 142, 32)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1\n",
        "        )  # params: (3*3*16*32+32) = 4640\n",
        "        self.conv4 = nn.Conv2d(\n",
        "            in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1\n",
        "        )  # params: (3*3*32*32+32) = 9248\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)  # Output shape = (None, 92, 71, 64)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # self.conv5 = nn.Conv2d(in_channels = 64, out_channels = 128,\n",
        "        # kernel_size = 3, stride = 1, padding = 1) # params: (3*3*32*32+32) = 9248\n",
        "        self.conv6 = nn.Conv2d(\n",
        "            in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1\n",
        "        )  # params: (3*3*32*32+32) = 9248\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)  # Output shape = (None, 46, 35, 128)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.do2 = nn.Dropout(0.3)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc1 = nn.Linear(\n",
        "            128 * 64 * 82, 60\n",
        "        )  # most recent original size of: 512, 662 -->64 x 82\n",
        "        self.do3 = nn.Dropout(0.4)  # 40 % probability\n",
        "        # self.fc3 = nn.Linear(60, 30)\n",
        "        self.fc2 = nn.Linear(60, 6)  # left with 3 for the three classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn1(self.pool1(F.relu(self.conv2(F.relu(self.conv1(x))))))\n",
        "        x = self.bn2(self.pool2(F.relu(self.conv4(F.relu(self.conv3(x))))))\n",
        "        # x = self.bn3(self.pool3(F.relu(self.conv6(F.relu(self.conv5(x))))))\n",
        "        x = self.bn3(self.pool3(F.relu(self.conv6((x)))))\n",
        "        x = self.do2(x)\n",
        "        x = x.view(x.size(0), 128 * 64 * 82)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.do3(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8J29SCjraCcu"
      },
      "outputs": [],
      "source": [
        "def train_val_model(epochs):\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        train_epoch_loss = 0\n",
        "        train_epoch_acc = 0\n",
        "\n",
        "        # set model in training mode\n",
        "        model.train()\n",
        "        print(\"\\nEpoch$ : %d\" % epoch)\n",
        "        for x_train_batch, y_train_batch in tqdm(train_loader):\n",
        "            x_train_batch = x_train_batch.to(\n",
        "                device\n",
        "            )  # (float).to(device) # for GPU support\n",
        "            y_train_batch = y_train_batch.to(device)\n",
        "\n",
        "            # print(x_train_batch.shape)\n",
        "\n",
        "            # sets gradients to 0 to prevent interference with previous epoch\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass through NN\n",
        "            y_train_pred = model(x_train_batch)  # .to(float)\n",
        "            train_loss = criterion(y_train_pred, y_train_batch)\n",
        "            train_acc = accuracy(y_train_pred, y_train_batch)\n",
        "\n",
        "            # Backward pass, updating weights\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            train_epoch_loss += train_loss.item()\n",
        "            train_epoch_acc += train_acc.item()\n",
        "\n",
        "        with torch.set_grad_enabled(False):\n",
        "            val_epoch_loss = 0\n",
        "            val_epoch_acc = 0\n",
        "\n",
        "            model.eval()\n",
        "            for x_val_batch, y_val_batch in tqdm(validate_loader):\n",
        "\n",
        "                x_val_batch = x_val_batch.to(device)  # .to(float)\n",
        "                y_val_batch = y_val_batch.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                y_val_pred = model(x_val_batch)  # .to(float)\n",
        "                val_loss = criterion(y_val_pred, y_val_batch)\n",
        "                val_acc = accuracy(y_val_pred, y_val_batch)\n",
        "\n",
        "                val_epoch_loss += val_loss.item()\n",
        "                val_epoch_acc += val_acc.item()\n",
        "\n",
        "        # Prevent plateauing validation loss\n",
        "        # scheduler.step(val_epoch_loss/len(validate_loader))\n",
        "\n",
        "        loss_stats[\"train\"].append(train_epoch_loss / len(train_loader))\n",
        "        loss_stats[\"val\"].append(val_epoch_loss / len(validate_loader))\n",
        "        accuracy_stats[\"train\"].append(train_epoch_acc / len(train_loader))\n",
        "        accuracy_stats[\"val\"].append(val_epoch_acc / len(validate_loader))\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch+0:03}: Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(validate_loader):.5f}\"\n",
        "        )\n",
        "        print(\n",
        "            f\"Train Acc: {train_epoch_acc/len(train_loader):.3f} | Val Acc: {val_epoch_acc/len(validate_loader):.3f}\"\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zE2oVsUSZgbl"
      },
      "outputs": [],
      "source": [
        " # Set to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lFAXymkCZ8aX",
        "outputId": "cfc8519a-9377-4ae7-eacb-c7f4d0f1724c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Model training on GPU\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 168MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RESNET Model training on GPU\n"
          ]
        }
      ],
      "source": [
        "# Get model\n",
        "model = set_model(m, model_ext, device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)#, momentum = 0.9) #or ADAM/ momentum\n",
        "# optimizer = torch.optim.SGD(\n",
        "#     model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
        "# )\n",
        "\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size = 4, gamma=0.1)\n",
        "# scheduler = ReduceLROnPlateau(optimizer, \"min\", patience=4)\n",
        "\n",
        "accuracy_stats = {\"train\": [], \"val\": []}\n",
        "\n",
        "loss_stats = {\"train\": [], \"val\": []}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "I5xpaSNBrNR5",
        "outputId": "193a311f-187d-4615-b480-870e5d3d8ee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch$ : 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/302 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipython-input-1179237093.py\", line 4, in collate_fn\n    return torch.utils.data.dataloader.default_collate(batch)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\", line 150, in collate\n    elem = batch[0]\n           ~~~~~^^^\nIndexError: list index out of range\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3815701182.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_val_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1734302121.py\u001b[0m in \u001b[0;36mtrain_val_model\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEpoch$ : %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx_train_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             x_train_batch = x_train_batch.to(\n\u001b[1;32m     12\u001b[0m                 \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1514\u001b[0m                 \u001b[0mworker_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1516\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1518\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data, worker_idx)\u001b[0m\n\u001b[1;32m   1549\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# be constructed, don't try to instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipython-input-1179237093.py\", line 4, in collate_fn\n    return torch.utils.data.dataloader.default_collate(batch)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\", line 150, in collate\n    elem = batch[0]\n           ~~~~~^^^\nIndexError: list index out of range\n"
          ]
        }
      ],
      "source": [
        "train_val_model(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SunqC9OGaO_r"
      },
      "outputs": [],
      "source": [
        "# added learning rate decay after 3rd epoch\n",
        "# 1st: round 1\n",
        "# 2nd: round 5\n",
        "# 3rd: round 6 best so far\n",
        "# 4th: round 7\n",
        "# 5th (model) saved training on 9\n",
        "# 6th: round 2\n",
        "# 7th round 8\n",
        "# 8th round 3\n",
        "# 9th round 4\n",
        "# 10th round 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHUjtLim2mdN"
      },
      "source": [
        "# Visualize the Training and Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wv3SoFHj2ssM",
        "outputId": "c3533faa-cdd3-4b4e-d228-05506e9384b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x700 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create dataframes\n",
        "train_val_acc_df = pd.DataFrame.from_dict(accuracy_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
        "train_val_loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs('/content/gdrive/MyDrive/Colab_Notebooks/acc_loss', exist_ok=True)\n",
        "os.makedirs('/content/gdrive/MyDrive/Colab_Notebooks/model_charts', exist_ok=True)\n",
        "\n",
        "train_val_acc_df.to_csv('/content/gdrive/MyDrive/Colab_Notebooks/acc_loss/acc{}.csv'.format(extension_), index = False)\n",
        "train_val_loss_df.to_csv('/content/gdrive/MyDrive/Colab_Notebooks/acc_loss/loss{}.csv'.format(extension_), index = False)\n",
        "# Plot the dataframes\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,7))\n",
        "sns.lineplot(data=train_val_acc_df, x = \"epochs\", y=\"value\", hue=\"variable\",  ax=axes[0]).set_title('Train-Val Accuracy/Epoch')\n",
        "sns.lineplot(data=train_val_loss_df, x = \"epochs\", y=\"value\", hue=\"variable\", ax=axes[1]).set_title('Train-Val Loss/Epoch')\n",
        "fig.savefig('/content/gdrive/MyDrive/Colab_Notebooks/model_charts/acc_loss{}.png'.format(extension_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITYjyxcK2a9P"
      },
      "source": [
        "# Evaluate the model using Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "uxkr2sXqqy6x",
        "outputId": "fc62f898-3c59-4f0b-f3a4-18ef67b8e7d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Accuracy.__new__() missing 1 required positional argument: 'task'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-771185602.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Calculate performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Accuracy.__new__() missing 1 required positional argument: 'task'"
          ]
        }
      ],
      "source": [
        "# Calculate performance\n",
        "y_test = torch.tensor([])\n",
        "test_acc = torchmetrics.Accuracy()\n",
        "\n",
        "with torch.set_grad_enabled(False):\n",
        "  model.eval()\n",
        "  #model.to(float)\n",
        "  for batches in tqdm(test_loader):\n",
        "    x_test, y_test = batches\n",
        "    x_test = x_test.to(device)\n",
        "    y_test = y_test.to(device)\n",
        "    y_pred = model(x_test)\n",
        "    test_acc(y_pred.cpu(), y_test.cpu())\n",
        "    total_test_acc = test_acc.compute()\n",
        "  print('test acc: ', total_test_acc)\n",
        "  test_acc.reset()\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4OOWwEWrW2-"
      },
      "source": [
        "## Create Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "QROmDemn_QeY",
        "outputId": "25f5f83e-083a-406a-8d0e-77a1d9eee533",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/254 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2069570038.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mx_test_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mx_test_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_test_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.to(float).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1179237093.py\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;34m\"\"\"From pytorch - way to bypass corrupt or non-existent data\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \"\"\"\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcollate\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0melem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0melem_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "all_pred = []\n",
        "all_preds = torch.tensor([])\n",
        "y_test = torch.tensor([])\n",
        "with torch.set_grad_enabled(False):\n",
        "  model.eval()\n",
        "  for x_test_batch, y_test_batch in tqdm(test_loader):\n",
        "    x_test_batch = x_test_batch.to(device)#.to(float).to(device)\n",
        "    y_test_pred = model(x_test_batch)\n",
        "    _, y_pred_probs = torch.max(y_test_pred, dim = 1)\n",
        "    all_pred.append(y_pred_probs.cpu().numpy())\n",
        "    all_preds = torch.cat((all_preds.cpu(), y_pred_probs.cpu()),dim = 0)\n",
        "    y_test = torch.cat((y_test, y_test_batch), dim = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "bI5PGwvU_SZq",
        "outputId": "04f74233-8562-4e46-da2f-eb36c9fe6d78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "zero-size array to reduction operation fmin which has no identity",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3293493553.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfusion_matrix_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.rename(columns=idx2class, index=idx2class)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".2f\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BuGn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label (ground truth)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/MyDrive/Colab_Notebooks/model_charts/CMTX{}.png'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextension_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36mheatmap\u001b[0;34m(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \"\"\"\n\u001b[1;32m    445\u001b[0m     \u001b[0;31m# Initialize the plotter object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m     plotter = _HeatMapper(data, vmin, vmax, cmap, center, robust, annot, fmt,\n\u001b[0m\u001b[1;32m    447\u001b[0m                           \u001b[0mannot_kws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbar_kws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticklabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                           yticklabels, mask)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;31m# Determine good default values for the colormapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         self._determine_cmap_params(plot_data, vmin, vmax,\n\u001b[0m\u001b[1;32m    164\u001b[0m                                     cmap, center, robust)\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36m_determine_cmap_params\u001b[0;34m(self, plot_data, vmin, vmax, cmap, center, robust)\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0mvmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0mvmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvmax\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrobust\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_nanfunctions_impl.py\u001b[0m in \u001b[0;36mnanmin\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;31m# Fast, but not safe for subclasses of ndarray, or object arrays,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;31m# which do not implement isnan (gh-9009), or fmin correctly (gh-8975)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             warnings.warn(\"All-NaN slice encountered\", RuntimeWarning,\n",
            "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation fmin which has no identity"
          ]
        }
      ],
      "source": [
        "confusion_matrix_df = pd.DataFrame(confusion_matrix(y_test, all_pred))#.rename(columns=idx2class, index=idx2class)\n",
        "sns.heatmap(confusion_matrix_df, annot=True, fmt=\".2f\", cmap='BuGn')\n",
        "plt.xlabel(\"prediction\")\n",
        "plt.ylabel(\"label (ground truth)\")\n",
        "plt.savefig('/content/gdrive/MyDrive/Colab_Notebooks/model_charts/CMTX{}.png'.format(extension_))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqMBs_YS_U0U"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "#class_names = [\"Possible Dementia\", \"Likely Dementia\", \"No Dementia\"]\n",
        "class_vals = [0,1,2]\n",
        "\n",
        "cr = classification_report(y_test, all_pred, class_vals, output_dict = True)\n",
        "try:\n",
        "    cr_file = open('/content/gdrive/MyDrive/Colab_Notebooks/model_charts/cr{}.txt'.format(extension_), 'wt')\n",
        "    cr_file.write(str(cr))\n",
        "    cr_file.close()\n",
        "\n",
        "except:\n",
        "    print(\"Unable to write to file\")\n",
        "print(classification_report(y_test, all_pred, class_vals))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhz6gXGHpcMB"
      },
      "source": [
        "# Plot ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nQrbRJ9_Xoh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "\n",
        "\n",
        "target= [0, 1, 2]\n",
        "\n",
        "# set plot figure size\n",
        "fig, c_ax = plt.subplots(1,1, figsize = (12, 8))\n",
        "\n",
        "# function for scoring roc auc score for multi-class\n",
        "def multiclass_roc_auc_score(y_test1, all_pred1, average=\"macro\"):\n",
        "    lb = LabelBinarizer()\n",
        "    lb.fit(y_test1)\n",
        "    y_test1 = lb.transform(y_test1)\n",
        "    all_pred1 = lb.transform(all_pred1)\n",
        "\n",
        "    for (idx, c_label) in enumerate(target):\n",
        "        fpr, tpr, thresholds = roc_curve(y_test1[:,idx].astype(int), all_pred1[:,idx])\n",
        "        c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
        "    c_ax.plot(fpr, fpr, 'b-', label = 'Random Guessing')\n",
        "    return roc_auc_score(y_test1, all_pred1, average=average)\n",
        "\n",
        "\n",
        "print('ROC AUC score:', multiclass_roc_auc_score(y_test, all_pred))\n",
        "\n",
        "c_ax.legend()\n",
        "c_ax.set_xlabel('False Positive Rate')\n",
        "c_ax.set_ylabel('True Positive Rate')\n",
        "plt.savefig('/content/gdrive/MyDrive/Colab_Notebooks/model_charts/roc{}.png'.format(extension_))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8DS8yc9YMQg"
      },
      "outputs": [],
      "source": [
        "# precision recall curve\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "\n",
        "# Use label_binarize to be multi-label like settings\n",
        "y = y_test.numpy()\n",
        "Y = label_binarize(y, classes=[0, 1, 2])\n",
        "Y_pred = label_binarize(all_pred, classes=[0, 1, 2])\n",
        "n_classes = Y.shape[1]\n",
        "\n",
        "precision = dict()\n",
        "recall = dict()\n",
        "for i in range(n_classes):\n",
        "    precision[i], recall[i], _ = precision_recall_curve(Y[:, i],\n",
        "                                                        Y_pred[:, i])\n",
        "    plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(i))\n",
        "\n",
        "plt.xlabel(\"recall\")\n",
        "plt.ylabel(\"precision\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.title(\"precision vs. recall curve\")\n",
        "plt.savefig('/content/gdrive/MyDrive/Colab_Notebooks/model_charts/auc_pr{}.png'.format(extension_))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKR_i_oebCi2"
      },
      "source": [
        "## Save the GPU CNN Model\n",
        "Also includes loading on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTEwjKcbbAlJ"
      },
      "outputs": [],
      "source": [
        "# Save GPU model\n",
        "model_name = 'cnn_1run.model{}'.format(extension_)\n",
        "PATH = \"/content/gdrive/MyDrive/Colab_Notebooks/{}\".format(model_name)\n",
        "torch.save(model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wm-xL-D6bcp6"
      },
      "outputs": [],
      "source": [
        "\"\"\"# Load GPU model\n",
        "device = torch.device(\"cuda\")\n",
        "model = TheModelClass(*args, **kwargs)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.to(device)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z8UDiRbLUG8"
      },
      "outputs": [],
      "source": [
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    class_names = [0,1,2]\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(validate_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                if j < len(preds):\n",
        "                    ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                    if inputs.cpu().data[j].shape[0] == 3:  # RGB\n",
        "                        imshow(inputs.cpu().data[j])\n",
        "                    else:  # Grayscale\n",
        "                        plt.imshow(inputs.cpu().data[j].squeeze(), cmap='gray')\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "visualize_model(model)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}